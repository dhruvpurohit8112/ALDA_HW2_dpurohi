{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-dd4b5746c67b8f50",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# **Problem 2 Knn & Adaboost (13 points) [TA: Vodelina Samatova]**\n",
    "\n",
    "In this part, you'll be working with KNN and AdaBoost. \n",
    "\n",
    "## Instructions:\n",
    "1. Use github.ncsu.edu to submit your work repository if you have not done so yet. It is your responsbility to ensure the TAs have access to your work before the deadline.\n",
    "2. Do not modify the code structure given. Answer the questions in the designated space.\n",
    "\n",
    "All the best!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fee3162f4d5bcd8a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# 0) Loading Data & Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4ce3395fb6fef52e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import datasets\n",
    "\n",
    "# set a seed for reproducibility\n",
    "random_seed = 25\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "# We need to ignore FutureWarnings due to a bug in our version of sklearn\n",
    "from warnings import simplefilter\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3a794107521856db",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# 1) YouTube Sentiment Analysis using KNN\n",
    "\n",
    "[Sentiment analysis](https://en.wikipedia.org/wiki/Sentiment_analysis) is the study of how we can systematically identify and quantify sentiment of a given segment of text. In this problem, you will be using the [YouTube Statistics](https://www.kaggle.com/datasets/advaypatil/youtube-statistics?select=comments.csv) dataset to build a classifier that will rate the sentiment of a string of text on a scale from 0 to 4.\n",
    "\n",
    "**WARNING:** This is *real* data from *real* people from YouTube. That means you might (and probably will) see some unscrupulous language when browsing the dataset. Don't browse with kids around!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-45c873099b66450c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's take a look at our data, and what attributes we have.\n",
    "\n",
    "* **Video ID**: The Video Identifier.\n",
    "* **Comment**: The comment text.\n",
    "* **Likes**: The number of likes the comment received.\n",
    "* **Sentiment**: The sentiment of the comment. A value of 0 represents a negative sentiment, while values of 1 or 2 represent neutral and positive sentiments respectively.\n",
    "\n",
    "For now, we only consider the **Comment** and **Sentiment** attributes.ly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('../data/hw2_p2_comments.csv')\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-15001b5e2db73bf0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Check our disribution of polarity\n",
    "# 0 means negative sentiment\n",
    "# 1 means neutral sentiment\n",
    "# 2 means positive sentiment\n",
    "train_data.Sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5327894d81868004",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1.1) Example: Brief Introduction to tf-idf\n",
    "\n",
    "Read through and run the following example, and answer the question at the end.\n",
    "\n",
    "In information retreival [tf-idf](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) (*term frequency â€“ inverse document frequency*) is a metric that represents how \"important\" a word is to a corpus of text. While we won't go into detail about how it works, essentially all you need to know is that it balances two metrics.\n",
    "\n",
    "**Term Frequency**: Is exactly what one would expect, it is the the frequency at which a word is present in a corpus of text. A word with a higher term-frequency score appears much more in the corpus compared to one with a low term frequency.\n",
    "\n",
    "**Inverse Document Frequency**: If we only used term frequency, common words like \"the\" or \"and\" would have a high score, even though they don't give us that much information since they are present in every document. *Inverse Document Frequency* is a metric of how much \"information\" a word provides, and if a word is common or rare across all documents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-1b74f76425851151",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### tf-idf with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?'\n",
    "]\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "#TODO: output the shape of X\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-6dc0a0bd2457d87c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The tf-idf vectorizer's `fit_transform` method returns a NxM matrix. `N` is the number of documents (sentences) you have in your corpus, and `M` is the number of unique words in your corpus. Item `n`x`m` is how important word `m` is to document `n`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Printing out the tf-idf matrix\n",
    "np.set_printoptions(precision=4)\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice that if we try and print X directly, we get an overview saying that X is a \"sparse matrix\".\n",
    "# In very large corpi with many unique words, a lot of row entries are going to consist of majority zeros\n",
    "# Thus numpy saves theses in a special compressed sparse format\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next let's see what word each column corresponds to:\n",
    "vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5bd2abaa3f0ad819",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's look at the tf-idf vectors for two different documents.\n",
    "\n",
    "**Note**: `dic(zip(A, B))` in pyton makes a dictionary out of a list of keys (A) and values (B). This just makes it easier to view each term with it's corresponding TFIDF value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b3627097c25aa7b0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print(corpus[0])\n",
    "dict(zip(vectorizer.get_feature_names_out(),X.toarray()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b72ce213380d8220",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print(corpus[1])\n",
    "dict(zip(vectorizer.get_feature_names_out(),X.toarray()[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-27d6d2b55e8301b3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Take a look at the tf-idf vectors for both of these sentences and answer the following questions:\n",
    "1. Why is the value for the term \"is\" higher in document1 than document2?\n",
    "2. Why is the value for the term \"document\" higher in document2 than document1?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer Here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5d6dd2a1fd26f181",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1.2) tf-idf on YouTube Data\n",
    "\n",
    "Now, before we build a classifier, let's just try and see what the nearest neighbors of a specified message are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-247861d851d2a5e9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Get our text\n",
    "corpus = train_data[\"Comment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values in your corpus\n",
    "corpus = corpus.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8a3dacaeb29d8337",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Run our transform\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tf = TfidfVectorizer()\n",
    "tfidf_matrix = tf.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2b47ac9333d7a0f3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Let's check the size of our matrix\n",
    "tfidf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-fcc831ed367bedc1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now, let's fit the nearest neighbors tree! Please use [NearestNeighbors](https://scikit-learn.org/stable/modules/neighbors.html) from sklearn library\n",
    "\n",
    "NOTE: fit the nearest neighbors tree (with **five** neighbors) on _**\"tfidf_matrx\"**_ we got from above, and return the model as _**\"nbrs\"**_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "nn",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "# TODO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "nn-public",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert(nbrs.n_features_in_ == 35685)\n",
    "assert(nbrs.n_samples_fit_ == 18408)\n",
    "assert(nbrs.get_params()['n_neighbors'] == 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b9db51af033047d2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We can now run custom sentences and see what sentences in the corpus are \"closest\" to what we put in. Try a few and see what shows up! In addition to this, you can change the `n_neighbors` param and get more queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a42677ceaef33de1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "test_docs = [\"I love this comedy movie\"]\n",
    "test_docs = tf.transform(test_docs)\n",
    "distances, indicies = nbrs.kneighbors(test_docs)\n",
    "train_data.iloc[indicies[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-12a84f0eac5cc6b1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# As a bonus, show our distances\n",
    "print(distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c0c7cdf55f67b06d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now investigate:\n",
    "1. Try manually classifying the tweet \"Wow, this is so cool!\" What are the classes of the neighbors? How would a 5-NN classifier classify that comment?\n",
    "2. Can you think of a comment that might fool this classifier? For example, how would it do with sarcasm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e38e9e04d7b092b3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# 2) AdaBoost\n",
    "\n",
    "In this exercise, you'll be learning how to use [AdaBoost](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html), as well as vizualize decision boundries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a2e82c5604cce2ec",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 2.1) Example: Decision Tree Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-643efca93cb2ef42",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Read the breast cancer dataset and translate to pandas dataframe\n",
    "bc_sk = datasets.load_breast_cancer()\n",
    "# Note that the \"target\" attribute is species, represented as an integer\n",
    "bc_data = pd.DataFrame(data= np.c_[bc_sk['data'], bc_sk['target']],columns= list(bc_sk['feature_names'])+['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3a7764bf66b2406b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# The fraction of data that will be test data\n",
    "test_data_fraction = 0.10\n",
    "\n",
    "bc_features = bc_data.iloc[:,0:-1]\n",
    "bc_labels = bc_data[\"target\"]\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(bc_features, bc_labels, test_size=test_data_fraction,  random_state=random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-41c236bfc6099552",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "First, let's have a baseline non-boosted decision tree to compare against."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e3cf16aea05cf2bd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "gini_tree = DecisionTreeClassifier(criterion = \"gini\", random_state=random_seed).fit(X=X_train, y=Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d5baa7e1ea2a864d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "predicted_y = gini_tree.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-3257c7df126745f6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print(classification_report(predicted_y,Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9cf042160ae42191",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "confusion_matrix(predicted_y,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2: Adaboost Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c930e798d27e97cd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now, let's get boosting with the [AdaBoostClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html). The default estimator for AdaBoost is a *decision stump*. (Remember: a decision stump is simply a decision tree with a height of 1).\n",
    "\n",
    "* The `n_estimators` parameter is the number of base models in the ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "adaboost",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# TODO: Create and train an AdaBoostClassifier with 20 estimators on the X_train and Y_train data\n",
    "# Note: Make sure to use random_state=random_seed\n",
    "ada_model = None\n",
    "\n",
    "# Fit the Adaboost classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-0f584b9b4cebda33",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "np.testing.assert_almost_equal(accuracy_score(ada_model.predict(X_test), Y_test), 0.9473684210526315)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5065de2c51aa8c2a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "predicted_y = ada_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-75ac58d9033a2843",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "print(classification_report(predicted_y,Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-16364b75ccc14537",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "confusion_matrix(predicted_y,Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-82768f17f1b3dd4e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "As we can see, the boosted model performs better than a full decision tree, even though it only uses some decision stumps. In the next section, we'll explore why that is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d52ea0f1fea6e677",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 2.3) Visualizing Decision Boundries\n",
    "\n",
    "Now, we're going to peek under the hood and see the decision boundries of ensemble learners. To do so, we'll implement our own version of ada boost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d0346937dddbd6fe",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-082cdb089cb68a59",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# A nice noisy dataset that's not linearly separable\n",
    "X,y = make_moons(300, noise=0.13,random_state = random_seed)\n",
    "colors = {0:'red',1:\"blue\"}\n",
    "plt.scatter(X[:,0],X[:,1],c=np.vectorize(colors.get)(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8685366fcdd41812",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 2.3.1: Calculating Alpha\n",
    "\n",
    "You're given some code to calculate Adaboost by hand below, but for it to work, you need to complete the following helper functions.\n",
    "\n",
    "In `calculate_alpha`, you should calculate alpha for the round, based on the classifier's predictions and the weights of that round. The basic steps are:\n",
    "1. Calculate the **weighted** error $\\epsilon$ for the round by comparing those predictions to y. Remember, the error is the sum of the weights of misclassified instances.\n",
    "2. Calculate alpha using the following formula:\n",
    "\n",
    "$$\\alpha = 0.5*ln(\\frac{1 - \\epsilon}{\\epsilon})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "calculate_alpha",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def calculate_alpha(predictions, weights, y):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        predictions: the predictions (0 or 1) of the classifier in this round.\n",
    "        weights: the weights of each instance at the end of last round\n",
    "        y: the class labels (0 or 1) for the instances\n",
    "    Output: the alpha value for this round\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2e4ddd7018e595b7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# To test calculate_alpha, we need to make a decision tree and some starting weights\n",
    "dt = DecisionTreeClassifier(criterion = \"gini\", max_depth=1, random_state=123).fit(X, y)\n",
    "predictions = dt.predict(X)\n",
    "weights = np.repeat(1 / len(X), len(X))\n",
    "# Alpha should be ~0.84\n",
    "alpha = calculate_alpha(predictions, weights, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "calculate_alpha-public",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "np.testing.assert_almost_equal(alpha, 0.8416209435087314)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-81feaeec97b698bd",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 2.3.2: Updating Weights\n",
    "\n",
    "Next you need to write a function that will update the weights for the adaboost classifier based on the alpha value. Remember:\n",
    "* Correctly classified instances have their weight decreased.\n",
    "* Incorrectly classified instances have their weight increased.\n",
    "\n",
    "The amount by which the weight changes is $e^\\alpha$.\n",
    "\n",
    "**Note**: You do not need to normalize the weights to sum to 1 - the code below will do that for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "update_weights",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def update_weights(predictions, weights, y, alpha):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        predictions: the predictions (0 or 1) of the classifier in this round.\n",
    "        weights: the weights at the end of the previous round for each instance\n",
    "        y: the class labels (0 or 1) for the instances\n",
    "        alpha: the alpha value for this round\n",
    "    Output: an array of updated weights for this round\n",
    "    \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize the weights to 1 / n\n",
    "weights = np.repeat(1 / len(X), len(X))\n",
    "alpha = calculate_alpha(predictions, weights, y)\n",
    "# Do one round of updating\n",
    "new_weights = update_weights(predictions, weights, y, alpha) \n",
    "# See how the weights have changed for each instance - some increased, some decreased\n",
    "print(new_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "update_weights-public",
     "locked": true,
     "points": 4,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "weights = np.repeat(1 / len(X), len(X))\n",
    "# Do one round of updating\n",
    "new_weights = update_weights(predictions, weights, y, alpha) \n",
    "alpha = 0.8416209435087314"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.testing.assert_almost_equal(max(new_weights), 1/300*math.exp(alpha))\n",
    "np.testing.assert_almost_equal(min(new_weights), 1/300/math.exp(alpha))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-f32f50069f5f8cff",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 2.3.3 Example: Adaboost Iteration\n",
    "\n",
    "Here's our code for Adaboost. You don't have to modify it, just finish the functions above, and then answer the free-response question at the end.\n",
    "\n",
    "Why are we implementing it by hand? This way we can keep track of which samples were chosen each round to visualize them.\n",
    "\n",
    "When you run the following code, you can see the alpha values for each round, changing as the weights, and therefore the samples, change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-054c5c4608d2ff88",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's do 20 rounds of adaboost\n",
    "rounds = 20\n",
    "# N is the length of training data\n",
    "N = len(X)\n",
    "# Each boostrapped sample will b size N/5\n",
    "sample_size = N // 5\n",
    "\n",
    "# Each round, keep track of:\n",
    "# Which instances (indices) were samples\n",
    "sample_indices = []\n",
    "# The classifier built on that sample\n",
    "dtrees = []\n",
    "# The alpha value for each that round\n",
    "alphas = []\n",
    "\n",
    "# Start our weights off each as 1/N\n",
    "weights = np.repeat(1 / N, N)\n",
    "\n",
    "for i in range(rounds):\n",
    "    indices = np.random.choice(range(N), sample_size, replace=True, p=weights)\n",
    "    sample_X = X[indices,:]\n",
    "    sample_y = y[indices]\n",
    "    # Train a simple decision tree on the sample\n",
    "    dt = DecisionTreeClassifier(criterion = \"gini\", max_depth=1).fit(sample_X, sample_y)\n",
    "    predictions = dt.predict(X)\n",
    "    \n",
    "    # Calculate alpha\n",
    "    alpha = calculate_alpha(predictions, weights, y)\n",
    "    print(f'Round {i} Alpha: {alpha}')\n",
    "    \n",
    "    # Update weights\n",
    "    weights = update_weights(predictions, weights, y, alpha)\n",
    "    weights /= sum(weights)\n",
    "    \n",
    "    sample_indices.append(indices)\n",
    "    alphas.append(alpha)\n",
    "    dtrees.append(dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-90951cf7e445ea45",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This function makes predictions for a given list of X features by taking the weighted sum of\n",
    "# its constituent classifiers' predictions.\n",
    "def boosting_predict(X_test):\n",
    "    return np.mean([(dtrees[i].predict(X_test) - 0.5) * alphas[i] for i in range(len(dtrees))], axis=0) + 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c52345667ce48266",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Based on the image above, how should points (0, 0) and (0, 1) be classified?\n",
    "# Note that the output is continuous: > 0.5 means more likely to be 1.\n",
    "boosting_predict([[0, 0], [0, 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-aa9e0d696d181de4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The following code will plot the predictions from individual rounds of adaboost. Note:\n",
    "* The background color indicates the prediction.\n",
    "* The faded dots were *not* sampled in this round (e.g. because of their low weights)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-51fe363d4a2d2ea8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now we plot the first 12 rounds of adaboost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is a provided method that will plot the decision boundries of each stump\n",
    "def plot_predictions(pred_function, sample_indices, subplot=plt, continuous=False):\n",
    "    plot_colors = [\"b\", \"r\"]  \n",
    "    plot_step = 0.02\n",
    "    class_names = \"AB\"\n",
    "\n",
    "    # Plot the decision boundaries\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n",
    "                         np.arange(y_min, y_max, plot_step))\n",
    "\n",
    "    Z = pred_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    if not continuous:\n",
    "        Z = np.round(Z)\n",
    "        cs = subplot.contourf(xx, yy, Z, cmap=plt.cm.Paired)\n",
    "    else:\n",
    "        cs = subplot.contourf(xx, yy, Z)\n",
    "\n",
    "    # Plot the training points\n",
    "    for i, n, c in zip(range(2), class_names, plot_colors):\n",
    "        idx = np.where(y == i)\n",
    "        in_sample_idx = np.intersect1d(idx, sample_indices)\n",
    "        subplot.scatter(X[in_sample_idx, 0], X[in_sample_idx, 1],\n",
    "                        color=c, s=20, edgecolor='k', label=\"Class %s (Sampled)\" % n)\n",
    "        \n",
    "        out_sample_idx = np.setdiff1d(idx, in_sample_idx)\n",
    "        subplot.scatter(X[out_sample_idx, 0], X[out_sample_idx, 1],\n",
    "                        color=c, s=20, edgecolor='k', alpha=0.1, label=\"Class %s (Not Sampled)\" % n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = 4\n",
    "cols = 3\n",
    "figure, axis = plt.subplots(rows, cols, figsize=(15, 10))\n",
    "figure.tight_layout()\n",
    "\n",
    "for i in range(rows):\n",
    "    for j in range(cols):\n",
    "        idx = i * cols + j  # Ensure correct indexing with cols\n",
    "        plot_predictions(dtrees[idx].predict, sample_indices[idx], axis[i, j])\n",
    "        axis[i, j].title.set_text(f'Round {idx}: Alpha = {alphas[idx]}')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-06bfa69d742f7d3f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now answer the following questions:\n",
    "1. Which rounds are most important to the classification? What do they have in common?\n",
    "2. Do you notice any changes in the samples (and resulting classifiers) in later rounds of Adaboost?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-4f4bb0b14d9893c9",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "**Answer here**\n",
    "1. \n",
    "2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e73e75b33d9ce2ae",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Finally, we can plot the whole adaboost classifier. Note the non-linear decision boundary.\n",
    "* The first plot shows the discrete boundary, comprised of all of the above weak classifiers.\n",
    "* The second plot shows the same prediction as a continuous boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-280cec5a4d32a015",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "plot_predictions(boosting_predict, range(len(X)), plt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-783d7129e43e87d9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "plot_predictions(boosting_predict, range(len(X)), plt, continuous=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7d7e77e2fb6c8cc5",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now answer the following questions:\n",
    "1. How can Adaboost create a non-linear boundary to accurately classify this shape?\n",
    "2. Which areas of the plot is the classifier most certain about, or least certain?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-25b0d6d39582ecb3",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "**Answer Here**\n",
    "1. \n",
    "2. "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
